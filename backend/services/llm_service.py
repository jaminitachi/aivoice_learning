import os
from openai import AsyncOpenAI

# --- í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ---
# .env íŒŒì¼ì—ì„œ LLM ê´€ë ¨ ì„¤ì •ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
OPENROUTER_API_KEY = os.getenv("LLM_API_KEY")
BASE_URL = os.getenv("LLM_BASE_URL", "https://openrouter.ai/api/v1")
MODEL_NAME = os.getenv("LLM_MODEL_NAME", "x-ai/grok-4-fast")


if not OPENROUTER_API_KEY:
    raise ValueError("LLM_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (OpenRouter API Key)")

# --- OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ---
# OpenRouter APIë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ base_urlì„ ì§€ì •í•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
client = AsyncOpenAI(
    base_url=BASE_URL,
    api_key=OPENROUTER_API_KEY,
)

def get_grok_client() -> AsyncOpenAI:
    """
    Grok LLM í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜
    í”¼ë“œë°± ì„œë¹„ìŠ¤ ë“± ë‹¤ë¥¸ ì„œë¹„ìŠ¤ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í´ë¼ì´ì–¸íŠ¸ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    return client

async def refine_stt_text(raw_stt_text: str) -> str:
    """
    STT ê²°ê³¼ë¥¼ ì •ì œí•©ë‹ˆë‹¤.
    - ê´„í˜¸ ì•ˆ ë‚´ìš© ì œê±° (ìŒí–¥ íš¨ê³¼, ë°°ê²½ìŒ ë“±)
    - ë°˜ë³µëœ ë‹¨ì–´/êµ¬ë¬¸ ì •ë¦¬
    - ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    - ê°„ë‹¨í•œ ë¬¸ë²• ë³´ì • (ëŒ€ì†Œë¬¸ì ë“±)
    
    Args:
        raw_stt_text: STT ì›ë³¸ í…ìŠ¤íŠ¸
    
    Returns:
        ì •ì œëœ í…ìŠ¤íŠ¸
    """
    import re
    
    # 1. ê¸°ë³¸ ì •ì œ (ë¹ ë¥¸ ê·œì¹™ ê¸°ë°˜)
    text = raw_stt_text.strip()
    
    # ê´„í˜¸ ì•ˆ ë‚´ìš© ì œê±° (ì˜ˆ: "[ìŒì•…]", "(ë°°ê²½ìŒ)", "[ì›ƒìŒ]" ë“±)
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'\(.*?\)', '', text)
    
    # ì—°ì†ëœ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    text = text.strip()
    
    # 2. LLMì„ ì‚¬ìš©í•œ ê³ ê¸‰ ì •ì œ (ë°˜ë³µ ì œê±°, ë¬¸ë§¥ìƒ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì •ë¦¬)
    try:
        refine_prompt = f"""Clean up this speech-to-text transcription. Remove:
- Repeated words or phrases (e.g., "I I think" â†’ "I think")
- Filler words that don't add meaning (um, uh, like) ONLY if excessive
- Any remaining transcription artifacts

Keep the natural conversational tone. Don't change the meaning.

Original: "{text}"

Return ONLY the cleaned text, nothing else."""

        response = await client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                {
                    "role": "system",
                    "content": "You are a text cleanup assistant. Return only the cleaned text, no explanations."
                },
                {
                    "role": "user",
                    "content": refine_prompt
                }
            ],
            max_tokens=300,
            temperature=0.3  # ë‚®ì€ ì˜¨ë„ë¡œ ì¼ê´€ì„± ìœ ì§€
        )
        
        refined = response.choices[0].message.content.strip()
        
        # ë”°ì˜´í‘œë¡œ ê°ì‹¸ì§„ ê²½ìš° ì œê±°
        if refined.startswith('"') and refined.endswith('"'):
            refined = refined[1:-1]
        
        print(f"ğŸ”§ STT Refined: '{raw_stt_text}' â†’ '{refined}'")
        return refined if refined else text
        
    except Exception as e:
        print(f"âš ï¸ STT ì •ì œ ì¤‘ ì˜¤ë¥˜ (ì›ë³¸ ë°˜í™˜): {e}")
        return text

async def get_llm_response(user_text: str, system_prompt: str, conversation_history: list = None) -> str:
    """
    OpenRouterë¥¼ í†µí•´ gpt-5-chat ëª¨ë¸ì„ í˜¸ì¶œí•˜ì—¬ AI ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í•¨ê»˜ ì „ë‹¬í•˜ì—¬ ë§¥ë½ì„ ìœ ì§€í•©ë‹ˆë‹¤.
    
    Args:
        user_text: í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€
        system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ìºë¦­í„° ì„¤ì •)
        conversation_history: ì´ì „ ëŒ€í™” íˆìŠ¤í† ë¦¬ [{"speaker": "user", "text": "..."}, ...]
    
    Returns:
        AI ì‘ë‹µ í…ìŠ¤íŠ¸
    """
    try:
        # ë©”ì‹œì§€ ë°°ì—´ êµ¬ì„±
        messages = [{"role": "system", "content": system_prompt}]
        
        # ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ìˆìœ¼ë©´ ì¶”ê°€ (ë§¥ë½ ìœ ì§€)
        if conversation_history:
            for msg in conversation_history:
                role = "user" if msg["speaker"] == "user" else "assistant"
                messages.append({"role": role, "content": msg["text"]})
        
        # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        messages.append({"role": "user", "content": user_text})
        
        print(f"\n{'ğŸ¤–'*30}")
        print(f"[LLM ì „ì²´ ë©”ì‹œì§€ í™•ì¸]")
        print(f"  ëª¨ë¸: {MODEL_NAME}")
        print(f"  ì´ ë©”ì‹œì§€ ìˆ˜: {len(messages)}ê°œ")
        for i, msg in enumerate(messages, 1):
            content_preview = msg['content'][:80] + "..." if len(msg['content']) > 80 else msg['content']
            print(f"  {i}. [{msg['role'].upper()}]: {content_preview}")
        print(f"{'ğŸ¤–'*30}\n")
        
        completion = await client.chat.completions.create(
            model=MODEL_NAME,
            messages=messages,
            max_tokens=100,
            temperature=0.7,
            extra_body={
                "reasoning": {
                    "enabled": True
                }
            }
        )
        ai_message = completion.choices[0].message.content
        return ai_message.strip() if ai_message else ""
    
    except Exception as e:
        # openai ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ë°œìƒí•˜ëŠ” ëª¨ë“  ì˜ˆì™¸ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        print(f"OpenRouter API í˜¸ì¶œ ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")
        # í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì „ë‹¬ë  ìµœì¢… ì˜ˆì™¸ ë©”ì‹œì§€ë¥¼ ì¼ê´€ë˜ê²Œ ê´€ë¦¬í•©ë‹ˆë‹¤.
        raise Exception("AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
